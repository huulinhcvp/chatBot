{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mnzYaT0ZUOvc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1591491296382,"user_tz":-420,"elapsed":23956,"user":{"displayName":"Đường Bạch Ngọc","photoUrl":"","userId":"14377386462041812856"}},"outputId":"b11f1f83-87e9-4058-98f2-ff06ba5ccfb3"},"source":["# Import Drive API and authenticate.\n","from google.colab import drive\n","\n","# Mount your Drive to the Colab VM.\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3DJ4guSwcZNI","colab_type":"code","colab":{}},"source":["!cp '/gdrive/My Drive/Project/attention.py' .\n","!cp '/gdrive/My Drive/Project/decoder.py' .\n","!cp '/gdrive/My Drive/Project/encoder.py' .\n","!cp '/gdrive/My Drive/Project/model.py' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcXHTuTbc7Hd","colab_type":"code","colab":{}},"source":["import os\n","import torch\n","import torchtext\n","from torch.nn.utils import clip_grad_norm_\n","from model import training\n","from datetime import datetime\n","import pickle\n","import collections"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9K_DZ1_dBlr","colab_type":"code","colab":{}},"source":["def check_dir_exists(path):\n","    if not os.path.isdir(path):\n","        os.makedirs(path)\n","\n","def save_object_to_Models(obj, path):\n","    check_dir_exists(os.path.dirname(path))\n","    with open(path, 'wb') as fd:\n","        pickle.dump(obj, fd)\n","\n","def save_model_to_Models(model, epoch, train_loss, val_loss):\n","    print('(Saving model...', end='')\n","    torch.save(model.state_dict(), '/gdrive/My Drive/Project/Models/' + ('seq2seq-%d-%f-%f.pt' % (epoch, train_loss, val_loss)))\n","    print('done)', end='')\n","\n","def save_vocab_to_Models(vocab, path):\n","    \n","    # Saves Torchtext Field vocabulary\n","    \n","    vocab.vectors = None\n","    save_object_to_Models(vocab, path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-lk38G6dWnw","colab_type":"code","colab":{}},"source":["def load_object(path):\n","    with open(path, 'rb') as fd:\n","        obj = pickle.load(fd)\n","    return obj\n","\n","def load_dataset(args, device):\n","    \"\"\"\n","    Loads field for twitter dataset.\n","    \"\"\"\n","    #'<sos>' --> start of sentence token\n","    #'<eos>' --> end of sentence token\n","    #'<pad>' --> pad token\n","    field = torchtext.data.Field(init_token='<sos>', eos_token='<eos>', pad_token='<pad>', tokenize='spacy', lower=True)\n","    dataset = 'applesupport'\n","\n","    train, val, test = torchtext.data.TabularDataset.splits(path='/gdrive/My Drive/Project/Data/', format='csv', train=dataset + '_train.csv', validation=dataset + '_val.csv', test=dataset + '_test.csv', fields=[('', None), ('author_id', None), ('question', field), ('answer', field)], skip_header=True)\n","    #field.build_vocab(train, vectors='glove.twitter.27B.200d', min_freq=2, max_size=20000)\n","    field.vocab = load_object('/gdrive/My Drive/Project/Models/' + 'vocab')\n","\n","    #field.build_vocab(train, vectors='glove.twitter.27B.200d', min_freq=2, max_size=20000)\n","    # create batchs for training\n","    train1, val1, test1 = torchtext.data.BucketIterator.splits((train, val, test), batch_size=args['batch_size'], sort_key=lambda x: len(x.question), device=device, repeat=False)\n","\n","    vocab = field.vocab\n","    Metadata = collections.namedtuple('Metadata', 'vocab_size padding_idx vectors')\n","    metadata = Metadata(vocab_size=len(vocab), padding_idx=vocab.stoi['<pad>'], vectors=vocab.vectors)\n","\n","    return metadata, field.vocab, train1, val1, test1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXCYLh41dXvx","colab_type":"code","colab":{}},"source":["def evaluate(model, val1, metadata):\n","    model.eval()  # put models in eval mode (this is important because of dropout)\n","\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch in val1:\n","            # calculate models predictions\n","            question, answer = batch.question, batch.answer\n","            logits = model(question, answer)\n","\n","            # calculate batch loss\n","            loss = torch.nn.functional.cross_entropy(logits.view(-1, metadata.vocab_size), answer[1:].view(-1),\n","                                   ignore_index=metadata.padding_idx)  # answer[1:] skip <sos> token\n","            total_loss += loss.item()\n","\n","    return total_loss / len(val1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T05sPgfLdouF","colab_type":"code","colab":{}},"source":["def train(model, optimizer, train1, metadata, grad_clip):\n","    model.train()\n","\n","    total_loss = 0\n","    for batch in train1:\n","        # models predictions\n","        question, answer = batch.question, batch.answer\n","        logits = model(question, answer)\n","\n","        optimizer.zero_grad()\n","\n","        # calculate loss and backpropagate errors\n","        loss = torch.nn.functional.cross_entropy(logits.view(-1, metadata.vocab_size), answer[1:].view(-1),\n","                               ignore_index=metadata.padding_idx)  # answer[1:] skip <sos> token\n","        loss.backward()\n","\n","        total_loss += loss.item()\n","\n","        # clip gradients to avoid exploding gradient\n","        clip_grad_norm_(model.parameters(), grad_clip)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","    return total_loss / len(train1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mM6EenQsLjza","colab_type":"code","colab":{}},"source":["def main():\n","\n","    cuda = False\n","    if torch.cuda.is_available():\n","      cuda = True\n","    \n","    torch.set_default_tensor_type(torch.cuda.FloatTensor if cuda else torch.FloatTensor)\n","    device = torch.device('cuda' if cuda else 'cpu')\n","    print(\"DEVICE = \", device)\n","\n","    hyper_parameter = {'epochs':300,\n","            'grad_clip':5,\n","            'batch_size':64,\n","            'learning_rate':1e-4,\n","            'path' : '/gdrive/My Drive/Project/Models/',\n","            'multi_gpu' : True,\n","            'encoder_hidden_size' : 1024,\n","            'encoder_num_layers' : 2,\n","            'encoder_rnn_dropout' : 0.2,\n","            'decoder_hidden_size' : 512,\n","            'decoder_num_layers' : 2,\n","            'decoder_rnn_dropout' : 0.2,\n","            'attn_hidden_size': 512,\n","            'embedding_size': 200,\n","            'cuda':cuda,}\n","\n","    if cuda:\n","        hyper_parameter['multi_gpu'] = True\n","    #print(\"My services = \", hyper_parameter['dataset'])\n","\n","    metadata, vocab, train1, val1, test1 = load_dataset(hyper_parameter, device)\n","    #print(list(train1))\n","    #print(list(val1))\n","\n","    \n","    print('Saving vocab and args...', end='')\n","    save_vocab_to_Models(vocab, hyper_parameter['path'] + os.path.sep + 'vocab')\n","    save_object_to_Models(hyper_parameter, hyper_parameter['path'] + os.path.sep + 'args')\n","    print('Done')\n","\n","    model = training(hyper_parameter, metadata)\n","    \n","    if cuda and hyper_parameter['multi_gpu']:\n","        model = torch.nn.DataParallel(model, dim=1)  # if we were using batch_first we'd have to use dim=0\n","        print('Use model DataParallel')\n","    model.load_state_dict(torch.load('/gdrive/My Drive/Project/Models/' + os.path.sep + 'seq2seq-63-2.229289-2.533210.pt'))\n","\n","    print(model)  # print models summary\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=hyper_parameter['learning_rate'], amsgrad=True)\n","\n","    try:\n","        for epoch in range(65, hyper_parameter['epochs']): \n","            begin = datetime.now()\n","            print('Training Epoch %d' % epoch)\n","            # calculate train and val loss\n","            train_loss = train(model, optimizer, train1, metadata, hyper_parameter['grad_clip'])\n","            val_loss = evaluate(model, val1, metadata)\n","            print(\"With epoch %d (%d): TRAIN-LOSS %f ** VALIDATION-LOSS %f (TIME = %s)\" % (epoch + 1, hyper_parameter['epochs'], train_loss, val_loss, datetime.now() - begin), end='')\n","\n","            # save models if models achieved best val loss (or save every epoch is selected)\n","            #if not best_val_loss or val_loss < best_val_loss:\n","            save_model_to_Models(model, epoch + 1, train_loss, val_loss)                \n","            print()\n","            print(\"***********************\")\n","    except (KeyboardInterrupt, BrokenPipeError):\n","        print('Training stopped!!!!!!!!!!!!!!!.')\n","\n","    test_loss = evaluate(model, test1, metadata)\n","    print(\"TEST-LOSS %f\" % test_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJNcMiRveSJ9","colab_type":"code","colab":{}},"source":["!cp '/gdrive/My Drive/Project/Data/amazonhelp_train.csv' .\n","!cp '/gdrive/My Drive/Project/Data/amazonhelp_val.csv' .\n","!cp '/gdrive/My Drive/Project/Data/amazonhelp_test.csv' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMAmIovDdpqZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591515767905,"user_tz":-420,"elapsed":24495447,"user":{"displayName":"Đường Bạch Ngọc","photoUrl":"","userId":"14377386462041812856"}},"outputId":"b8d06971-5280-4816-9705-e6413a92bf64"},"source":["main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["DEVICE =  cuda\n","Saving vocab and args...Done\n","Use model DataParallel\n","DataParallel(\n","  (module): SequenceToSequenceTrain(\n","    (encoder): Encoder(\n","      (embed): Embedding(10998, 200, padding_idx=1)\n","      (rnn): RNN(\n","        (rnn): LSTM(200, 1024, num_layers=2, dropout=0.2, bidirectional=True)\n","      )\n","    )\n","    (decoder): Decoder(\n","      (initial_hidden): DecoderInit(\n","        (linear): Linear(in_features=1024, out_features=512, bias=True)\n","      )\n","      (embed): Embedding(10998, 200, padding_idx=1)\n","      (rnn): LSTM(712, 512, num_layers=2, dropout=0.2)\n","      (attn): Attention(\n","        (attn_score): generalAttentionScore()\n","      )\n","      (attn_hidden_lin): Linear(in_features=2560, out_features=512, bias=True)\n","      (out): Linear(in_features=512, out_features=10998, bias=True)\n","    )\n","  )\n",")\n","Training Epoch 65\n","With epoch 66 (300): TRAIN-LOSS 2.214210 ** VALIDATION-LOSS 2.549388 (TIME = 0:17:06.519266)(Saving model...done)\n","***********************\n","Training Epoch 66\n","With epoch 67 (300): TRAIN-LOSS 2.212901 ** VALIDATION-LOSS 2.592538 (TIME = 0:17:08.124834)(Saving model...done)\n","***********************\n","Training Epoch 67\n","With epoch 68 (300): TRAIN-LOSS 2.189364 ** VALIDATION-LOSS 2.550199 (TIME = 0:17:03.951995)(Saving model...done)\n","***********************\n","Training Epoch 68\n","With epoch 69 (300): TRAIN-LOSS 2.185067 ** VALIDATION-LOSS 2.617971 (TIME = 0:17:07.693261)(Saving model...done)\n","***********************\n","Training Epoch 69\n","With epoch 70 (300): TRAIN-LOSS 2.181167 ** VALIDATION-LOSS 2.609212 (TIME = 0:17:07.215580)(Saving model...done)\n","***********************\n","Training Epoch 70\n","With epoch 71 (300): TRAIN-LOSS 2.151184 ** VALIDATION-LOSS 2.646619 (TIME = 0:17:05.807943)(Saving model...done)\n","***********************\n","Training Epoch 71\n","With epoch 72 (300): TRAIN-LOSS 2.134492 ** VALIDATION-LOSS 2.591443 (TIME = 0:17:09.697971)(Saving model...done)\n","***********************\n","Training Epoch 72\n","With epoch 73 (300): TRAIN-LOSS 2.118756 ** VALIDATION-LOSS 2.662237 (TIME = 0:17:08.464247)(Saving model...done)\n","***********************\n","Training Epoch 73\n","With epoch 74 (300): TRAIN-LOSS 2.114610 ** VALIDATION-LOSS 2.613943 (TIME = 0:17:09.300294)(Saving model...done)\n","***********************\n","Training Epoch 74\n","With epoch 75 (300): TRAIN-LOSS 2.098712 ** VALIDATION-LOSS 2.647920 (TIME = 0:17:09.702469)(Saving model...done)\n","***********************\n","Training Epoch 75\n","With epoch 76 (300): TRAIN-LOSS 2.088492 ** VALIDATION-LOSS 2.627616 (TIME = 0:17:09.531960)(Saving model...done)\n","***********************\n","Training Epoch 76\n","With epoch 77 (300): TRAIN-LOSS 2.055844 ** VALIDATION-LOSS 2.715603 (TIME = 0:17:06.785833)(Saving model...done)\n","***********************\n","Training Epoch 77\n","With epoch 78 (300): TRAIN-LOSS 2.041568 ** VALIDATION-LOSS 2.693295 (TIME = 0:17:07.853502)(Saving model...done)\n","***********************\n","Training Epoch 78\n","With epoch 79 (300): TRAIN-LOSS 2.032186 ** VALIDATION-LOSS 2.776293 (TIME = 0:17:09.031694)(Saving model...done)\n","***********************\n","Training Epoch 79\n","With epoch 80 (300): TRAIN-LOSS 1.993151 ** VALIDATION-LOSS 2.792022 (TIME = 0:17:10.880263)(Saving model...done)\n","***********************\n","Training Epoch 80\n","With epoch 81 (300): TRAIN-LOSS 1.995977 ** VALIDATION-LOSS 2.833602 (TIME = 0:17:08.768362)(Saving model...done)\n","***********************\n","Training Epoch 81\n","With epoch 82 (300): TRAIN-LOSS 1.964824 ** VALIDATION-LOSS 2.847126 (TIME = 0:17:11.964184)(Saving model...done)\n","***********************\n","Training Epoch 82\n","With epoch 83 (300): TRAIN-LOSS 1.933123 ** VALIDATION-LOSS 2.857545 (TIME = 0:17:10.656260)(Saving model...done)\n","***********************\n","Training Epoch 83\n","With epoch 84 (300): TRAIN-LOSS 1.930263 ** VALIDATION-LOSS 2.858688 (TIME = 0:17:08.064517)(Saving model...done)\n","***********************\n","Training Epoch 84\n","With epoch 85 (300): TRAIN-LOSS 1.911208 ** VALIDATION-LOSS 2.911727 (TIME = 0:17:09.789173)(Saving model...done)\n","***********************\n","Training Epoch 85\n","With epoch 86 (300): TRAIN-LOSS 1.882566 ** VALIDATION-LOSS 2.918248 (TIME = 0:17:13.397444)(Saving model...done)\n","***********************\n","Training Epoch 86\n","With epoch 87 (300): TRAIN-LOSS 1.856070 ** VALIDATION-LOSS 2.979244 (TIME = 0:17:12.402957)(Saving model...done)\n","***********************\n","Training Epoch 87\n","With epoch 88 (300): TRAIN-LOSS 1.825656 ** VALIDATION-LOSS 3.008548 (TIME = 0:17:11.318032)(Saving model...done)\n","***********************\n","Training Epoch 88\n","Training stopped!!!!!!!!!!!!!!!.\n","TEST-LOSS 3.057834\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"05fOhnRDwENH","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}